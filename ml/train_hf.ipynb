{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import ast\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ast(example):\n",
    "    example[\"ner_tags\"] = ast.literal_eval(example[\"ner_tags\"])\n",
    "    example[\"tokens\"] = ast.literal_eval(example[\"tokens\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"data/train.csv\")[\"train\"]\n",
    "dataset = dataset.remove_columns(\"processed_text\")\n",
    "dataset = dataset.remove_columns(\"target_labels_positions\")\n",
    "dataset = dataset.remove_columns(\"label\")\n",
    "dataset = dataset.remove_columns(\"strat\")\n",
    "dataset = dataset.train_test_split(test_size=0.15, seed=42)\n",
    "dataset = dataset.rename_column(\"label_ids\", \"ner_tags\")\n",
    "dataset = dataset.rename_column(\"processed_text_split\", \"tokens\")\n",
    "dataset = dataset.map(add_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ner_tags', 'tokens'],\n",
       "        num_rows: 423\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ner_tags', 'tokens'],\n",
       "        num_rows: 75\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ai-forever/sbert_large_mt_nlu_ru\"\n",
    "output_dir = \"baseline_ai-forever-sbert_large_mt_nlu_ru_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        # Map tokens to their respective word.\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Only label the first token of a given word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-discount\": 1,\n",
    "    \"B-value\": 2,\n",
    "    \"I-value\": 3\n",
    "}\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-discount\",\n",
    "    2: \"B-value\",\n",
    "    3: \"I-value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def weighted_f1(y_true, y_pred, weights):\n",
    "    \"\"\"Расчет взвешенной F1 меры с индивидуальными весами для классов.\"\"\"\n",
    "\n",
    "    y_true_flat = []\n",
    "    y_pred_flat = []\n",
    "\n",
    "    for prediction, label in zip(y_pred, y_true):\n",
    "        for pr, le in zip(prediction, label):\n",
    "            if le == -100:\n",
    "                continue\n",
    "            else:\n",
    "                y_true_flat.append(le)\n",
    "                y_pred_flat.append(pr)\n",
    "\n",
    "    # рассчитываем F1 для всех классов\n",
    "    _, _, f1, support = precision_recall_fscore_support(y_true_flat, y_pred_flat, average=None,\n",
    "                                                        labels=list(weights.keys()))\n",
    "\n",
    "    # вычисление взвешенной F-меры\n",
    "    weighted_f1 = np.sum(f1 * [weights[label] for label in np.unique(y_true_flat + y_pred_flat)] * support) / np.sum(\n",
    "        support * [weights[label] for label in np.unique(y_true_flat + y_pred_flat)])\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    class_weights = {\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        3: 2,\n",
    "        0: 0\n",
    "    }\n",
    "\n",
    "    weighted_f1_score = weighted_f1(labels, predictions, class_weights)\n",
    "\n",
    "    return {\n",
    "        \"f1\": weighted_f1_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/sbert_large_mt_nlu_ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=4, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91df8c410e9942ab80e9ce7804830885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ca120aeff34bc181d572b169066b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.019432175904512405, 'eval_f1': 0.9937436839649448, 'eval_runtime': 1.6049, 'eval_samples_per_second': 46.731, 'eval_steps_per_second': 6.231, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aae80ef85ce4b64928803e5dd867e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01454983837902546, 'eval_f1': 0.9955436337784025, 'eval_runtime': 1.4435, 'eval_samples_per_second': 51.958, 'eval_steps_per_second': 6.928, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61db59bed4c42108554b35908275a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.013076773844659328, 'eval_f1': 0.9964641339285212, 'eval_runtime': 1.6476, 'eval_samples_per_second': 45.521, 'eval_steps_per_second': 6.069, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50311139f1484b83ab08984ca42513a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014544963836669922, 'eval_f1': 0.9966027706646867, 'eval_runtime': 1.6193, 'eval_samples_per_second': 46.316, 'eval_steps_per_second': 6.175, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eefed70eabc4249aa6abd6ec7568178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014852426946163177, 'eval_f1': 0.9966573178086795, 'eval_runtime': 1.6134, 'eval_samples_per_second': 46.486, 'eval_steps_per_second': 6.198, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ac58f3be4a4758b47cf6d9195f0c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.016773777082562447, 'eval_f1': 0.9966475517719255, 'eval_runtime': 1.6365, 'eval_samples_per_second': 45.83, 'eval_steps_per_second': 6.111, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1741730479b8471a89ffefc6801830da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018464187160134315, 'eval_f1': 0.9965485264843058, 'eval_runtime': 1.6372, 'eval_samples_per_second': 45.809, 'eval_steps_per_second': 6.108, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbbf2517b9e468690e2ac61b2af1b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01902492716908455, 'eval_f1': 0.996792208239845, 'eval_runtime': 1.6453, 'eval_samples_per_second': 45.584, 'eval_steps_per_second': 6.078, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f85b944b72447eeb3ae00c591427fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.019655821844935417, 'eval_f1': 0.9965762074408555, 'eval_runtime': 1.6197, 'eval_samples_per_second': 46.305, 'eval_steps_per_second': 6.174, 'epoch': 9.0}\n",
      "{'loss': 0.0134, 'grad_norm': 0.11675046384334564, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27f919ac7f247d8a53856b902cbeb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01983506791293621, 'eval_f1': 0.9968583886315955, 'eval_runtime': 1.642, 'eval_samples_per_second': 45.676, 'eval_steps_per_second': 6.09, 'epoch': 10.0}\n",
      "{'train_runtime': 351.0664, 'train_samples_per_second': 12.049, 'train_steps_per_second': 1.51, 'train_loss': 0.012717333991291388, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=530, training_loss=0.012717333991291388, metrics={'train_runtime': 351.0664, 'train_samples_per_second': 12.049, 'train_steps_per_second': 1.51, 'total_flos': 3816559961389224.0, 'train_loss': 0.012717333991291388, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "trainer.save_model(os.path.join(output_dir, \"final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
